1. Create an S3 Bucket
    aws --endpoint-url=http://localhost:4566 s3 mb s3://csv-bucket
2. Create a DynamoDB Table
    import boto3

def create_dynamodb_table():
    dynamodb = boto3.client('dynamodb', endpoint_url="http://localhost:4566")
    table_name = 'CSV_Metadata'
    
    try:
        dynamodb.create_table(
            TableName=table_name,
            KeySchema=[
                {'AttributeName': 'filename', 'KeyType': 'HASH'},  # Partition key
            ],
            AttributeDefinitions=[
                {'AttributeName': 'filename', 'AttributeType': 'S'},
            ],
            ProvisionedThroughput={
                'ReadCapacityUnits': 5,
                'WriteCapacityUnits': 5,
            }
        )
        print(f"Table {table_name} created successfully!")
    except Exception as e:
        print(f"Error creating table: {e}")

if __name__ == "__main__":
    create_dynamodb_table()
3. Implement the Lambda Function
import boto3
import pandas as pd
import os
from datetime import datetime

s3_client = boto3.client('s3', endpoint_url="http://localhost:4566")
dynamodb = boto3.resource('dynamodb', endpoint_url="http://localhost:4566")
table = dynamodb.Table('CSV_Metadata')

def lambda_handler(event, context):
    # Get bucket name and file key from the event
    bucket_name = event['Records'][0]['s3']['bucket']['name']
    file_key = event['Records'][0]['s3']['object']['key']
    
    # Download the file
    download_path = f"/tmp/{file_key}"
    s3_client.download_file(bucket_name, file_key, download_path)
    
    # Process the CSV
    try:
        df = pd.read_csv(download_path)
        metadata = {
            'filename': file_key,
            'upload_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'file_size_bytes': os.path.getsize(download_path),
            'row_count': len(df),
            'column_count': len(df.columns),
            'column_names': list(df.columns),
        }
        
        # Store metadata in DynamoDB
        table.put_item(Item=metadata)
        print(f"Metadata stored successfully for {file_key}")
    
    except Exception as e:
        print(f"Error processing file {file_key}: {e}")
        raise e

Here's a detailed step-by-step guide to completing the assignment, from setup to coding, with clear instructions on where to run commands, where to code, and how to organize your files.

Step 1: Set Up the Environment
Install Localstack and Tools
Open your terminal (Command Prompt, Terminal on macOS/Linux, or PowerShell).

Install Localstack:

bash
Copy code
pip install localstack
Start Localstack:

bash
Copy code
localstack start
Install AWS CLI:

bash
Copy code
pip install awscli
Configure AWS CLI: Run the following command in the terminal:

bash
Copy code
aws configure
Access Key: test
Secret Key: test
Region: us-east-1
Output format: json
Install Boto3 and Pandas
In the terminal, install the required Python libraries:

bash
Copy code
pip install boto3 pandas
Directory Structure
Create a folder to organize your project files:

graphql
Copy code
cloud-engineer-assignment/
├── app/
│   ├── lambda_function.py  # Lambda code
│   ├── utils.py            # Helper functions
│   ├── requirements.txt    # Dependencies for Lambda
├── database/
│   ├── setup_dynamodb.py   # Script for DynamoDB setup
│   ├── setup_rds.py        # Script for RDS setup
├── scripts/
│   ├── s3_upload.py        # Script for uploading CSV to S3
├── README.md               # Documentation
Step 2: Create and Configure AWS Resources in Localstack
1. Create an S3 Bucket
Run the following commands in your terminal:

bash
Copy code
aws --endpoint-url=http://localhost:4566 s3 mb s3://csv-bucket
2. Create a DynamoDB Table
Write a Python script to create the DynamoDB table:

File: database/setup_dynamodb.py

python
Copy code
import boto3

def create_dynamodb_table():
    dynamodb = boto3.client('dynamodb', endpoint_url="http://localhost:4566")
    table_name = 'CSV_Metadata'
    
    try:
        dynamodb.create_table(
            TableName=table_name,
            KeySchema=[
                {'AttributeName': 'filename', 'KeyType': 'HASH'},  # Partition key
            ],
            AttributeDefinitions=[
                {'AttributeName': 'filename', 'AttributeType': 'S'},
            ],
            ProvisionedThroughput={
                'ReadCapacityUnits': 5,
                'WriteCapacityUnits': 5,
            }
        )
        print(f"Table {table_name} created successfully!")
    except Exception as e:
        print(f"Error creating table: {e}")

if __name__ == "__main__":
    create_dynamodb_table()
Run the script:

bash
Copy code
python database/setup_dynamodb.py
Step 3: Implement the Lambda Function
1. Write the Lambda Code
Create the Lambda function to process the CSV file.

File: app/lambda_function.py

python
Copy code
import boto3
import pandas as pd
import os
from datetime import datetime

s3_client = boto3.client('s3', endpoint_url="http://localhost:4566")
dynamodb = boto3.resource('dynamodb', endpoint_url="http://localhost:4566")
table = dynamodb.Table('CSV_Metadata')

def lambda_handler(event, context):
    # Get bucket name and file key from the event
    bucket_name = event['Records'][0]['s3']['bucket']['name']
    file_key = event['Records'][0]['s3']['object']['key']
    
    # Download the file
    download_path = f"/tmp/{file_key}"
    s3_client.download_file(bucket_name, file_key, download_path)
    
    # Process the CSV
    try:
        df = pd.read_csv(download_path)
        metadata = {
            'filename': file_key,
            'upload_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'file_size_bytes': os.path.getsize(download_path),
            'row_count': len(df),
            'column_count': len(df.columns),
            'column_names': list(df.columns),
        }
        
        # Store metadata in DynamoDB
        table.put_item(Item=metadata)
        print(f"Metadata stored successfully for {file_key}")
    
    except Exception as e:
        print(f"Error processing file {file_key}: {e}")
        raise e
4. Deploy Lambda Function to Localstack
  aws --endpoint-url=http://localhost:4566 lambda create-function \
    --function-name process_csv \
    --runtime python3.9 \
    --role arn:aws:iam::000000000000:role/lambda-role \
    --handler lambda_function.lambda_handler \
    --zip-file fileb://app/lambda_function.zip \
    --timeout 60
5. Set Up S3 Event Trigger
aws --endpoint-url=http://localhost:4566 s3api put-bucket-notification-configuration \
    --bucket csv-bucket \
    --notification-configuration '{
        "LambdaFunctionConfigurations": [
            {
                "LambdaFunctionArn": "arn:aws:lambda:us-east-1:000000000000:function:process_csv",
                "Events": ["s3:ObjectCreated:*"]
            }
        ]
    }'
6. Upload a CSV File to Test
import boto3

def upload_file_to_s3(file_path, bucket_name):
    s3 = boto3.client('s3', endpoint_url="http://localhost:4566")
    file_name = file_path.split("/")[-1]
    s3.upload_file(file_path, bucket_name, file_name)
    print(f"Uploaded {file_name} to {bucket_name}")

if __name__ == "__main__":
    upload_file_to_s3("path/to/your/file.csv", "csv-bucket")
7.python scripts/s3_upload.py
8.Verify Results
aws --endpoint-url=http://localhost:4566 dynamodb scan --table-name CSV_Metadata
9.aws --endpoint-url=http://localhost:4566 logs describe-log-groups
